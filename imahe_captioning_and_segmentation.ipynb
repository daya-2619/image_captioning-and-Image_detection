{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers -qq\n",
        "!pip install Pillow -qq\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# --- Content of vision_module.py from cell FZ8Cgc_7g9t3 ---\n",
        "vision_module_content = \"\"\"\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from transformers import (\n",
        "    BlipProcessor,\n",
        "    BlipForConditionalGeneration,\n",
        "    DetrImageProcessor,\n",
        "    DetrForObjectDetection,\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Singleton Model Loader\n",
        "# -----------------------------\n",
        "class Models:\n",
        "    caption_processor = None\n",
        "    caption_model = None\n",
        "    detection_processor = None\n",
        "    detection_model = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Load Caption Model\n",
        "# -----------------------------\n",
        "def load_caption_model(model_name=\"Salesforce/blip-image-captioning-base\"):\n",
        "    if Models.caption_model is None:\n",
        "        print(f\"Loading caption model: {model_name} on {Models.device}\")\n",
        "        Models.caption_processor = BlipProcessor.from_pretrained(model_name)\n",
        "        Models.caption_model = BlipForConditionalGeneration.from_pretrained(model_name).to(Models.device)\n",
        "    return Models.caption_processor, Models.caption_model\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Load Detection Model\n",
        "# -----------------------------\n",
        "def load_detection_model(model_name=\"facebook/detr-resnet-50\"):\n",
        "    if Models.detection_model is None:\n",
        "        print(f\"Loading detection model: {model_name} on {Models.device}\")\n",
        "        Models.detection_processor = DetrImageProcessor.from_pretrained(model_name)\n",
        "        Models.detection_model = DetrForObjectDetection.from_pretrained(model_name).to(Models.device)\n",
        "    return Models.detection_processor, Models.detection_model\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Caption Image\n",
        "# -----------------------------\n",
        "def generate_caption(image_pil, max_length=30, num_beams=3):\n",
        "    proc, model = load_caption_model()\n",
        "    inputs = proc(images=image_pil, return_tensors=\"pt\").to(Models.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(**inputs, max_length=max_length, num_beams=num_beams)\n",
        "\n",
        "    caption = proc.decode(out_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Object Detection\n",
        "# -----------------------------\n",
        "def detect_objects(image_pil, threshold=0.5):\n",
        "    proc, model = load_detection_model()\n",
        "    inputs = proc(images=image_pil, return_tensors=\"pt\").to(Models.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    target_sizes = torch.tensor([image_pil.size[::-1]])\n",
        "    results = proc.post_process_object_detection(\n",
        "        outputs, threshold=threshold, target_sizes=target_sizes\n",
        "    )[0]\n",
        "\n",
        "    detections = []\n",
        "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "        detections.append({\n",
        "            \"label\": model.config.id2label[label.item()],\n",
        "            \"score\": float(score),\n",
        "            \"box\": [float(x) for x in box.tolist()],\n",
        "        })\n",
        "    return detections\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Draw Caption + Detection Boxes\n",
        "# -----------------------------\n",
        "def draw_annotations(image_pil, caption, detections):\n",
        "    img = image_pil.copy()\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 16)\n",
        "    except:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    # Draw caption bar\n",
        "    W, H = img.size\n",
        "    draw.rectangle([(0, 0), (W, 35)], fill=\"black\")\n",
        "    draw.text((10, 8), caption, fill=\"white\", font=font)\n",
        "\n",
        "    # Boxes\n",
        "    for d in detections:\n",
        "        x0, y0, x1, y1 = d[\"box\"]\n",
        "        label = f\"{d['label']} {d['score']:.2f}\"\n",
        "\n",
        "        draw.rectangle([x0, y0, x1, y1], outline=\"red\", width=3)\n",
        "        draw.text((x0, y0 - 12), label, fill=\"red\", font=font)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Main processing function\n",
        "# -----------------------------\n",
        "def analyze_image(image_path):\n",
        "    image_pil = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    caption = generate_caption(image_pil)\n",
        "    detections = detect_objects(image_pil)\n",
        "    annotated = draw_annotations(image_pil, caption, detections)\n",
        "\n",
        "    return caption, detections, annotated\n",
        "\"\"\"\n",
        "# --------------------------------------------------\n",
        "\n",
        "# Write the content to a file so it can be imported\n",
        "with open(\"vision_module.py\", \"w\") as f:\n",
        "    f.write(vision_module_content)\n",
        "\n",
        "# Now import the module\n",
        "from vision_module import analyze_image\n",
        "\n",
        "# Ensure the image for the example is available\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/4/47/European_shorthair_-_Carl_the_cat.jpg\"\n",
        "image_path_local = \"/content/european-shorthair-8601492_640.jpg\"\n",
        "\n",
        "if not os.path.exists(image_path_local):\n",
        "    print(f\"Downloading image from {image_url} to {image_path_local}\")\n",
        "    response = requests.get(image_url)\n",
        "    response.raise_for_status() # Raise an exception for HTTP errors\n",
        "    with open(image_path_local, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "else:\n",
        "    print(f\"Image already exists at {image_path_local}\")\n",
        "\n",
        "# Execute the original code\n",
        "caption, detections, annotated = analyze_image(image_path_local)\n",
        "caption, detections, annotated\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaXQghRRx6M3",
        "outputId": "73368eda-6456-4584-cdca-c894ce64c977"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image already exists at /content/european-shorthair-8601492_640.jpg\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('a kitten with blue eyes laying on a green blanket',\n",
              " [{'label': 'cat',\n",
              "   'score': 0.9986379742622375,\n",
              "   'box': [55.34730911254883,\n",
              "    18.42893409729004,\n",
              "    639.542724609375,\n",
              "    383.47613525390625]}],\n",
              " <PIL.Image.Image image mode=RGB size=640x427>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}